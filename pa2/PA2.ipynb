{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LKeNxBw1ori",
    "outputId": "7b87decb-8c5d-4e2f-db1a-9b129d6551f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-addons in /Users/adrian/opt/anaconda3/lib/python3.9/site-packages (0.18.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/adrian/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /Users/adrian/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-addons) (22.0)\n",
      "Requirement already satisfied: ffmpeg in /Users/adrian/opt/anaconda3/lib/python3.9/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu\n",
    "!pip install tensorflow-addons\n",
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpYiU9AF1_gW"
   },
   "source": [
    "C1 to C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zIlBdfga17Wf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 14:04:04.635560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/context.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m _b\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m x:x\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m message \u001b[38;5;28;01mas\u001b[39;00m _message\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reflection \u001b[38;5;28;01mas\u001b[39;00m _reflection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import ffmpeg\n",
    "import gzip\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "class MMNISTDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data_set):\n",
    "        self.data_set = data_set\n",
    "        self.batch_size = 16\n",
    "        \n",
    "        if data_set == \"train\":\n",
    "          self.length = int(9008/self.batch_size)\n",
    "          f = gzip.open('/Desktop/comp4211/pa2/pa2_data/train-images-idx3-ubyte.gz','r')\n",
    "          image_size = 28\n",
    "          num_images = 60000\n",
    "          f.read(16)\n",
    "          buf = f.read(image_size * image_size * num_images)\n",
    "          self.mnist_train_im  = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "          self.mnist_train_im  = self.mnist_train_im.reshape(num_images, image_size, image_size, 1)\n",
    "        elif data_set == \"test\":\n",
    "          self.length = int(9008/self.batch_size)\n",
    "          test_set = np.load('/Desktop/comp4211/pa2/pa2_data/MNIST_test_seq.npy')\n",
    "          test_set = test_set.transpose(1,0,2,3)\n",
    "          self.mnist_test = test_set[0:9008]\n",
    "          self.mnist_test = np.expand_dims(self.mnist_test, axis=4)\n",
    "        elif data_set == \"valid\":\n",
    "          self.length = int(992/self.batch_size)\n",
    "          test_set = np.load('/Desktop/comp4211/pa2/pa2_data/MNIST_test_seq.npy')\n",
    "          test_set = test_set.transpose(1,0,2,3)\n",
    "          self.mnist_valid = test_set[9008:10000]\n",
    "          self.mnist_valid = np.expand_dims(self.mnist_valid, axis=4)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def gen_random_sequence(self):\n",
    "        # randomly generate a sequence of a digit\n",
    "        size = 64 - 28\n",
    "        x, y, theta = np.random.random(), np.random.random(), np.random.random() * 2 * np.pi\n",
    "        velocity_y, velocity_x = np.sin(theta), np.cos(theta)\n",
    "        seq_x, seq_y = np.zeros(20), np.zeros(20)\n",
    "\n",
    "        for i in range(20):\n",
    "            y += 0.1*velocity_y\n",
    "            x += 0.1*velocity_x\n",
    "        \n",
    "            if x <= 0: \n",
    "              x=0\n",
    "              velocity_x = -velocity_x\n",
    "            if x >= 1.0:\n",
    "              x = 1.0\n",
    "              velocity_x = -velocity_x\n",
    "            if y <= 0:\n",
    "              y=0\n",
    "              velocity_y = -velocity_y\n",
    "            if y >= 1.0:\n",
    "              y = 1.0\n",
    "              velocity_y = -velocity_y\n",
    "            seq_x[i], seq_y[i] = x, y\n",
    "            # Scale to the size.\n",
    "            seq_x = (size * seq_x).astype(np.int32)\n",
    "            seq_y = (size * seq_y).astype(np.int32)\n",
    "            return seq_y, seq_x\n",
    "\n",
    "    def random_mmnist(self):\n",
    "        # generate frames of moving mnist\n",
    "        data = np.zeros((20, 64, 64, 1), dtype=np.float32)\n",
    "        # 10 input + 10 groundtruth = 20\n",
    "        for n in range(2):\n",
    "            seq_y, seq_x = self.gen_random_sequence()\n",
    "            idx = np.random.randint(0, self.mnist_train_im.shape[0] - 1)\n",
    "            mnist_image = self.mnist_train_im[idx]\n",
    "            for i in range(20):\n",
    "                # put the 2 moving digits into \"data\"\n",
    "                data[i, seq_y[i]:seq_y[i]+28, seq_x[i]:seq_x[i]+28] =\\\n",
    "                    np.maximum(\n",
    "                    data[i, seq_y[i]:seq_y[i]+28, seq_x[i]:seq_x[i]+28],\n",
    "                    mnist_image\n",
    "                    )\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      if self.data_set == 'train':\n",
    "        a = np.expand_dims(self.random_mmnist(), axis=0)\n",
    "        for i in range(15):\n",
    "          a = np.append(a, np.expand_dims(self.random_mmnist(), axis=0), axis=0)\n",
    "        a = a/255\n",
    "        input = a[:, 0:10]\n",
    "        gt = a[:, 10:20]\n",
    "      elif self.data_set == 'test':\n",
    "        temp = self.mnist_test\n",
    "        temp = temp/255\n",
    "        input = temp[idx, 0:10]\n",
    "        gt = temp[idx, 10:20]\n",
    "      else:\n",
    "        temp = self.mnist_valid\n",
    "        temp = temp/255\n",
    "        input = temp[idx, 0:10]\n",
    "        gt = temp[idx, 10:20]\n",
    "\n",
    "      return input, gt\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5erpoGCfeCA"
   },
   "source": [
    "C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uRve9qrpLcZc"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.skip_connection = None\n",
    "    self.enc = [\n",
    "        Enc_ConvB(64,(1,1)), # as the same H & W\n",
    "        Enc_ConvB(64,(2,2)), # as output with H/2 & W/2\n",
    "        Enc_ConvB(64,(1,1)), # as the same with above one\n",
    "        Enc_ConvB(64,(2,2)) # as output with H/4 & W/4\n",
    "    ]\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.reshape(x, shape=(160, 64, 64, 1))\n",
    "    x = self.enc[0](x)\n",
    "    self.skip_connection = x\n",
    "    x = self.enc[1](x)\n",
    "    x = self.enc[2](x)\n",
    "    x = self.enc[3](x)\n",
    "    x = tf.reshape(x, shape=(16, 10, 64, 64, 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-9llk-XfgtY"
   },
   "source": [
    "C5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "pxLhFFy8tjyL"
   },
   "outputs": [],
   "source": [
    "class Enc_ConvB(tf.keras.layers.Layer):\n",
    "  def __init__(self, out_channels_size, stride_size):\n",
    "      super(Enc_ConvB, self).__init__()\n",
    "      self.conv = tf.keras.layers.Conv2D(\n",
    "          filters=out_channels_size, \n",
    "          strides=stride_size, \n",
    "          kernel_size=(3,3), \n",
    "          padding='same')\n",
    "      self.norm = tfa.layers.GroupNormalization(groups=2, \n",
    "                                                epsilon=1e-5)\n",
    "      self.act = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.act(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rtnkimJfjLj"
   },
   "source": [
    "Q1\n",
    "\n",
    "s_1 = 1 as same H(64)&W(64) for output\n",
    "\n",
    "s_2 = 2 as H/2(32)&W/2(32) for output\n",
    "\n",
    "s_3 = 1 as same H/2(32)&W/2(32) for output\n",
    "\n",
    "s_4 = 2 as H/4(16)&W/4(16) for output\n",
    "\n",
    "e.g. [(64+2*1-3)/s_2 + 1] = 32, therefore s_2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSExh9d1fkkK"
   },
   "source": [
    "Q2\n",
    "The trainable parameters = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2Clt594fnu4"
   },
   "source": [
    "C6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ILgMDUTRuJ5j"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(Translator, self).__init__()\n",
    "    self.enc = [\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256),\n",
    "        Inception(256)\n",
    "    ]\n",
    "    self.dec = [\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(512),\n",
    "        Inception(640)\n",
    "    ]\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x.transpose(0,3,1,2,4)\n",
    "    x = tf.reshape(x, shape=(16, 16, 16, 10*64))\n",
    "    IncepEnc_1 = self.enc[0](x)\n",
    "    IncepEnc_2 = self.enc[1](IncepEnc_1,256)\n",
    "    IncepEnc_3 = self.enc[2](IncepEnc_2,256)\n",
    "    IncepEnc_4 = self.enc[3](IncepEnc_3,256)\n",
    "    IncepEnc_5 = self.enc[4](IncepEnc_4,256)\n",
    "    IncepEnc_6 = self.enc[5](IncepEnc_5,256)\n",
    "    IncepEnc_7 = self.enc[6](IncepEnc_6,256)\n",
    "    IncepEnc_8 = self.enc[7](IncepEnc_7,256)\n",
    "\n",
    "    IncepDec_1 = self.dec[0](IncepEnc_8,256)\n",
    "    temp = np.concatenate((IncepDec_1, IncepEnc_7), axis=3)\n",
    "    IncepDec_2 = self.dec[1](temp,256)\n",
    "    temp = np.concatenate((IncepDec_2, IncepEnc_6), axis=3)\n",
    "    IncepDec_3 = self.dec[2](temp,256)\n",
    "    temp = np.concatenate((IncepDec_3, IncepEnc_5), axis=3)\n",
    "    IncepDec_4 = self.dec[3](temp,256)\n",
    "    temp = np.concatenate((IncepDec_4, IncepEnc_4), axis=3)\n",
    "    IncepDec_5 = self.dec[4](temp,256)\n",
    "    temp = np.concatenate((IncepDec_5, IncepEnc_3), axis=3)\n",
    "    IncepDec_6 = self.dec[5](temp,256)\n",
    "    temp = np.concatenate((IncepDec_6, IncepEnc_2), axis=3)\n",
    "    IncepDec_7 = self.dec[6](temp,256)\n",
    "    temp = np.concatenate((IncepDec_7, IncepEnc_1), axis=3)\n",
    "    IncepDec_8 = self.dec[7](temp,640)\n",
    "\n",
    "    x = IncepDec_8.reshape(16,16,16,10,64)\n",
    "    x = x.transpose(0,2,3,1,4)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdN4LmSJfpYc"
   },
   "source": [
    "C7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tIqtUvR2uJ3f"
   },
   "outputs": [],
   "source": [
    "class Inception(tf.keras.layers.Layer):\n",
    "  def __init__(self, C_out):\n",
    "    super(Inception, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(\n",
    "        filters=128, \n",
    "        strides=1, \n",
    "        kernel_size=(1,1)) # as the same H & W\n",
    "    self.layers = [\n",
    "        GroupConv(C_out,(3,3)),\n",
    "        GroupConv(C_out,(5,5)),\n",
    "        GroupConv(C_out,(7,7)),\n",
    "        GroupConv(C_out,(11,11))\n",
    "    ]\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    y1 = self.layers[0](x)\n",
    "    y2 = self.layers[1](x)\n",
    "    y3 = self.layers[2](x)\n",
    "    y4 = self.layers[3](x)\n",
    "    return (y1+y2+y3+y4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGEg4pTSfsDo"
   },
   "source": [
    "Q3\n",
    "\n",
    "The C_in:\n",
    "\n",
    "for IncepEnc1 = 640\n",
    "\n",
    "for IncepEnc2,3,4,5,6,7,8 & IncepDec1 = 256\n",
    "\n",
    "for IncepDec2,3,4,5,6,7,8 = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3W1_KFEftEV"
   },
   "source": [
    "Q4\n",
    "\n",
    "The size should be 1 as output the same H'&W'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYgJSp3Sfuv6"
   },
   "source": [
    "C8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BvUBTnyguJ1I"
   },
   "outputs": [],
   "source": [
    "class GroupConv(tf.keras.layers.Layer):\n",
    "  def __init__(self, C_out, k):\n",
    "    super(GroupConv, self).__init__()\n",
    "    self.conv = tf.keras.layers.Conv2D(filters=C_out, strides=(1,1), kernel_size=k, padding='same', groups=8)\n",
    "    self.norm = tfa.layers.GroupNormalization(groups=8, epsilon=1e-5)\n",
    "    self.act = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.act(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aBAvZSXfwsO"
   },
   "source": [
    "Q5\n",
    "\n",
    "The number of times = 16*4 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXlWSHC7fx1A"
   },
   "source": [
    "Q6\n",
    "\n",
    "The C_out = 256 or 640\n",
    "The kernal size = (3,3), (5,5), (7,7), (11,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9BgG4-Sfzo4"
   },
   "source": [
    "C9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "C_0jyau8ubZq"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, skip_connection):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.skip_connection = skip_connection\n",
    "    self.dec = [\n",
    "        Dec_ConvB(64, (2,2), True), # double x & y\n",
    "        Dec_ConvB(64, (1,1), False), # same x & y\n",
    "        Dec_ConvB(64, (2,2), True), # double x & y\n",
    "        Dec_ConvB(64, (1,1), False) # same x & y\n",
    "    ]\n",
    "    self.readout = tf.keras.layers.Conv2D(1, kernel_size = (1,1))\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = tf.reshape(x, shape=(16*10, 16, 16, 64))\n",
    "    x = self.dec[0](x)\n",
    "    x = self.dec[1](x)\n",
    "    x = self.dec[2](x)\n",
    "    x = np.concatenate((x, self.skip_connection), axis=3)\n",
    "    x = self.dec[3](x)\n",
    "    x = self.readout(x)\n",
    "    x = tf.reshape(x, shape=(16, 10, 64, 64, 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mmMzYxf2Oy"
   },
   "source": [
    "C10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "OEN-7nABubXh"
   },
   "outputs": [],
   "source": [
    "class Dec_ConvB(tf.keras.layers.Layer):\n",
    "  def __init__(self, C_out, s, transpose):\n",
    "    super(Dec_ConvB, self).__init__()\n",
    "    self.transpose = transpose\n",
    "    if transpose == True:\n",
    "      self.conv = tf.keras.layers.Conv2DTranspose(filters=C_out, strides=s, kernel_size=(3,3), padding='same', output_padding = (1,1))\n",
    "    else:\n",
    "      self.conv = tf.keras.layers.Conv2D(filters=C_out, strides=s, kernel_size=(3,3), padding='same')\n",
    "    self.norm = tfa.layers.GroupNormalization(groups=2, epsilon=1e-5)\n",
    "    self.act = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.norm(x)\n",
    "    x = self.act(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAW-Dua0f7hy"
   },
   "source": [
    "Q7\n",
    "\n",
    "The values are 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLg39m4Jf8R9"
   },
   "source": [
    "Q8\n",
    "\n",
    "The s_1 = (2,2)\n",
    "The s_2 = (1,1)\n",
    "The s_3 = (2,2)\n",
    "The s_4 = (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE9KeY69f9EN"
   },
   "source": [
    "Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bcxym-P2ubVJ"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.enc = Encoder()\n",
    "    self.hid = Translator()\n",
    "    self.dec = Decoder(self.enc.skip_connection)\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    x = self.hid(x)\n",
    "    x = self.dec(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaQxGQUyf-rL"
   },
   "source": [
    "C11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_NaO3JxAsPSH"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(Model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QtgrNTcCsPO5"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=['mse', 'mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0ilk9Rzu68wM"
   },
   "outputs": [],
   "source": [
    "train_set = MMNISTDataset('train')\n",
    "test_set = MMNISTDataset('test')\n",
    "valid_set = MMNISTDataset('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "ceTVZz6VuJoU",
    "outputId": "005a2970-bbfa-4a72-86a7-624d4707ba50"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f119a358fdff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-d563183843ac>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-5d99f2b5da7b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"encoder_2\" \"                 f\"(type Encoder).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 40960 values, but the requested shape has 655360 [Op:Reshape]\n\nCall arguments received by layer \"encoder_2\" \"                 f\"(type Encoder):\n  • x=tf.Tensor(shape=(10, 64, 64, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjXBS8vbDs_C"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/MyDrive/Colab Notebooks/pa2_data/pretrained_main/pretrained_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shZRldEW2wnv"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1npvYm62wlL"
   },
   "outputs": [],
   "source": [
    "# model.fit(train_set, epochs=10, validation_data = valid_set)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
